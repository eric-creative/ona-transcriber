{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# !pip install datasets\n",
    "# !pip install transformers\n",
    "# !pip install huggingface\n",
    "# !pip install ffmpeg-python\n",
    "# !pip install bitsandbytes accelerate loralib"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# importing the model\n",
    "model_name = \"openai/whisper-small\"\n",
    "language = \"sw\"\n",
    "task = \"transcribe\"\n",
    "dataset_name = \"whispere\"\n",
    "\n",
    "output_file_name = \"/models\""
   ],
   "id": "1c59e83813267dee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq, WhisperForConditionalGeneration, WhisperTokenizer, WhisperProcessor, AutomaticSpeechRecognitionPipeline\n",
    "from transformers import pipeline\n",
    "import ffmpeg\n",
    "import torch\n",
    "\n",
    "\n",
    "# Using a pipeline as a high-level helper\n",
    "\n",
    "pipe = pipeline(\"automatic-speech-recognition\", model=model_name)"
   ],
   "id": "c2e7c11154501477",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load model directly\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(model_name, local_files_only=True)"
   ],
   "id": "863320d8360d37a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "whisper_asr = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model_name,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    chunk_length_s=30,\n",
    ")"
   ],
   "id": "8b975bc67a89c0ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def format_time(seconds):\n",
    "    hours = int(seconds / 3600)\n",
    "    minutes = int((seconds % 3600) // 60)\n",
    "    seconds = int(seconds % 60)\n",
    "    return f\"{hours:02}:{minutes:02}:{seconds:02}\".replace('.',',')"
   ],
   "id": "3aa7b22f43fab89d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def process_audio_and_create_vtt(audio_file_name, audio_type, whisper_asr):\n",
    "    prediction = whisper_asr(f\"{audio_file_name}.{audio_type}\", return_timestamps=True)\n",
    "    \n",
    "    vtt_file = output_file_name if output_file_name else f\"{audio_file_name}.vtt\"\n",
    "    \n",
    "    with open(vtt_file, \"w\", encoding='utf-8') as f:\n",
    "        f.write(f\"ONAVTT\\n\\n\")\n",
    "        for chunk in prediction['chunks']:\n",
    "            start, end = chunk['timestamp']\n",
    "            start_time = format_time(start)\n",
    "            end_time = format_time(end)\n",
    "            text = chunk['text']\n",
    "            f.write(f\"{start_time} --> {end_time}\\n{text}\\n\\n\")"
   ],
   "id": "90862932b7a9afa2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Now processing the audios and providing the transcripts\n",
    "process_audio_and_create_vtt('./mp3/train', 'mp3', whisper_asr=whisper_asr)"
   ],
   "id": "49bced21ecd11235",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
